{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Popularity of Video Games\n",
    "based on number of reviews in the Steam game store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "The goal of this project was to create a linear regression model using scraped data to properly describe and predict video game popularity based on number of reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data for this project was scraped from the Steam Game Store <https://store.steampowered.com>\n",
    "A combination of Selenium and BeautifulSoup was used to scrape 16 thousand different pages for features such as:\n",
    "<ul>\n",
    "    <li>Release Date</li>\n",
    "    <li>Price of Game</li>\n",
    "    <li>If Game Has Been on Sale</li>\n",
    "    <li>Publisher of Game</li>\n",
    "    <li>Genre</li>\n",
    "    <li>Lagnuages Offered</li>\n",
    "    <li>Rating</li>\n",
    "    <li>User Assigned Descriptive Tags</li>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "Several jupyter notebooks have been included (with descriptions) in this repo demonstrating the different methods used. Selenium was used on all scraped pages, causing the total scraping time to be over elevn hours. Much of the data is categorical, but also stored as lits in each row. Because of this, creating dummy columns was not as easy as calling the pd.get_dummies function. Instead, a total list of values was created. Then each list was analized to count instances of each term using .Counter, sorted by frequency using .Counter.most_often, and the top 30 terms were manually turned into feature columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models\n",
    "\n",
    "Initial modeling of the data with limited features returned an r squared of just .03. The model needed more complexity. After all categorical data was added, PolynomialFeatures was used to raise complexity, as well as create feature interactions. Lasso Regularization was then used to select only important features and prevent overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Final scores for the model:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
