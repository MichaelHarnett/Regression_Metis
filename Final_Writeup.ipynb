{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Popularity of Video Games\n",
    "based on number of reviews in the Steam game store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "The goal of this project was to create a linear regression model using scraped data to properly describe and predict video game popularity based on number of reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The data for this project was scraped from the Steam Game Store <https://store.steampowered.com>\n",
    "A combination of Selenium and BeautifulSoup was used to scrape 16 thousand different pages for features such as:\n",
    "<ul>\n",
    "    <li>Release Date</li>\n",
    "    <li>Price of Game</li>\n",
    "    <li>If Game Has Been on Sale</li>\n",
    "    <li>Publisher of Game</li>\n",
    "    <li>Genre</li>\n",
    "    <li>Lagnuages Offered</li>\n",
    "    <li>Rating</li>\n",
    "    <li>User Assigned Descriptive Tags</li>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms\n",
    "\n",
    "Several jupyter notebooks have been included (with descriptions) in this repo demonstrating the different methods used. Selenium was used on all scraped pages, causing the total scraping time to be over elevn hours. Much of the data is categorical, but also stored as lits in each row. Because of this, creating dummy columns was not as easy as calling the pd.get_dummies function. Instead, a total list of values was created. Then each list was analized to count instances of each term using .Counter, sorted by frequency using .Counter.most_often, and the top 30 terms were manually turned into feature columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models\n",
    "\n",
    "Initial modeling of the data with limited features returned an r squared of just .03. The model needed more complexity. After looing at the historgram of the target variable, and looking at the error metrics, it was clear that the data was very heavily skewed. Becuase of this, log transofrmations were taken of the target variable and other numerical features. While this cots some interpretability, predictive power was the goal for this project.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Final scores for the model:\n",
    "After performing several transformations, error cost was able to be reduced greatly. Both mean absolute error and root mean squared error are below 1, and the r^2 for the model increased from .05, to .8. Logarithmic transformations were performed on the data that sacrificed interpretability in terms of feature interactions and importance, however, the model's errors have been minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Forward \n",
    "Try time series? SteamSpy does show onwerhsip information by year "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
